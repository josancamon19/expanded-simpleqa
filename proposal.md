### Variant A — Benchmark / dataset framing (closest to *SimpleQA*-style)

**Abstract.** Short-form factuality remains difficult to evaluate at scale: older Wikipedia-linked resources such as **WikiQA** focus on answer-sentence selection and contain on the order of a few thousand questions, limiting coverage and headroom for modern models. ([Microsoft][1]) More recent benchmarks like **SimpleQA** emphasize *single, indisputable answers* and easy grading, but are still constrained in size (4,326 questions) and require substantial filtering for reliability (e.g., **SimpleQA Verified**, 1,000 prompts). ([OpenAI CDN][2]) We introduce **Graph-Grounded SimpleQA (GG-SimpleQA)**, a pipeline to construct a **large, verifiable short-answer factuality benchmark** from Wikipedia by leveraging explicit structure. We first generate per-article knowledge graphs using **KGGen**, a text-to-KG generator that reduces graph sparsity via entity clustering and de-duplication. ([arXiv][3]) We then adapt a **STaRK-inspired** semi-structured synthesis procedure to propose candidate fact-seeking questions whose answers correspond to specific nodes/relations in the extracted graph, yielding automatically grounded question–answer pairs. ([arXiv][4]) To ensure benchmark quality, we apply a two-stage verification protocol: (i) targeted human review on ambiguity and answerability, and (ii) an LLM judge that checks whether the reference answer is *explicitly supported by the source article* and rejects underspecified items. Scaling to **10,000 articles × ~5 questions/article**, GG-SimpleQA targets **~50,000** graded, short-answer questions—**1–2 orders of magnitude larger** than prior short-form factuality sets—enabling finer-grained measurement and more robust model comparisons. 

---

### Variant B — Method + experiments framing (closest to *pipeline + ablations* pitch)

**Abstract.** We study whether **graph-mediated synthesis** can scale the creation of **high-reliability short-answer factuality evaluations** beyond current benchmarks. SimpleQA demonstrates that short, fact-seeking questions with **single, indisputable answers** can be graded cleanly, but its construction remains bottlenecked by curation (4,326 questions) and subsequent filtering for reliability (e.g., SimpleQA Verified, 1,000 prompts). ([OpenAI CDN][2]) We propose a structure-first alternative: given a Wikipedia article, we generate an article-level knowledge graph with **KGGen**, which clusters and resolves entities to reduce sparsity and improve downstream usability. ([arXiv][3]) From this graph, we produce candidate factual questions using a **STaRK-inspired** semi-structured query synthesis approach that integrates relational constraints with textual grounding, yielding questions that map to specific graph entities/edges with deterministic reference answers. ([arXiv][4]) We then validate each item via a hybrid protocol: minimal human review for ambiguity plus an LLM-based judge that verifies **answer presence and support** in the original article, rejecting items that are not directly answerable from the provided context. Our target scale is **~50,000 questions** (10,000 articles × 5 questions), enabling two core experiments: (1) **direct prompting** (article → QA) versus **graph-mediated generation** (article → KG → QA) to test whether explicit structure improves answerability and reduces ambiguity; and (2) **memorization-oriented training comparisons**, contrasting standard supervised fine-tuning (SFT) with KL-regularized approaches under fixed compute to quantify gains in parametric factuality on the resulting benchmark. The outcome is a scalable recipe for producing large, clean, and auditable factuality evaluations grounded in Wikipedia.


[1]: https://www.microsoft.com/en-us/download/details.aspx?id=52419&utm_source=chatgpt.com "Download Microsoft Research WikiQA Corpus from Official ..."
[2]: https://cdn.openai.com/papers/simpleqa.pdf?utm_source=chatgpt.com "Measuring short-form factuality in large language models"
[3]: https://arxiv.org/abs/2502.09956?utm_source=chatgpt.com "KGGen: Extracting Knowledge Graphs from Plain Text with Language Models"
[4]: https://arxiv.org/abs/2404.13207?utm_source=chatgpt.com "STaRK: Benchmarking LLM Retrieval on Textual and ..."
[5]: https://web.stanford.edu/class/cs197/assignments/project.html "CS197 | Project"
